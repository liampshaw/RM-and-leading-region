
```{r}
library(ggplot2)
library(knitr)
library(xtable)
library(dplyr)
library(tidyr)
```

## Compare hard shell genes vs. accessory


### Short palindromes (k=4, 6)

```{r}
# Pangenome statistics for PTUs
ptu = read.csv('../data/PTU-pangenome-stats.csv')

# Read in RMES comparisons
df_4 = read.csv('~/Downloads/trieste/test_k4.csv')
df_6 = read.csv('~/Downloads/trieste/test_k6.csv')
df_4_filter = df_4[df_4$PTU %in% ptu$PTU[ptu$pangenome.category=="include"],]
df_6_filter = df_6[df_6$PTU %in% ptu$PTU[ptu$pangenome.category=="include"],]
```

Do the statistical test

```{r}
wilcox.test(df_4_filter$discrepancy_selected, df_4_filter$discrepancy_others, paired = TRUE)
wilcox.test(df_6_filter$discrepancy_selected, df_6_filter$discrepancy_others, paired = TRUE)

```
```{r}
ggplot(df_4_filter, aes(x="k=4", wilcoxon_stat, colour=p_value<0.05))+
  geom_point()
```

### All RM targets (not included in main text)

```{r }
df_4 = read.csv('~/Downloads/trieste/test_4mer_targets.csv')
df_4_filter = df_4[df_4$PTU %in% ptu$PTU[ptu$pangenome.category=="include"],]
df_5 = read.csv('~/Downloads/trieste/test_5mer_targets.csv')
df_5_filter = df_5[df_5$PTU %in% ptu$PTU[ptu$pangenome.category=="include"],]
df_6 = read.csv('~/Downloads/trieste/test_6mer_targets.csv')
df_6_filter = df_6[df_6$PTU %in% ptu$PTU[ptu$pangenome.category=="include"],]

wilcox_tests = list(wilcox.test(df_4_filter$discrepancy_selected, df_4_filter$discrepancy_others, paired = TRUE),
                    wilcox.test(df_5_filter$discrepancy_selected, df_5_filter$discrepancy_others, paired = TRUE),
                    wilcox.test(df_6_filter$discrepancy_selected, df_6_filter$discrepancy_others, paired = TRUE))
results_df = data.frame(median_discrepancy_targets=c(median(df_4_filter$discrepancy_selected),
                                        median(df_5_filter$discrepancy_selected),
                                        median(df_6_filter$discrepancy_selected)),
           median_discrepancy_others=c(median(df_4_filter$discrepancy_others),
                                        median(df_5_filter$discrepancy_others),
                                        median(df_6_filter$discrepancy_others)),
           wilcoxon_stat=c(wilcox_tests[[1]]$statistic,
                           wilcox_tests[[2]]$statistic,
                           wilcox_tests[[3]]$statistic),
           p_value=c(wilcox_tests[[1]]$p.value,
                     wilcox_tests[[2]]$p.value,
                     wilcox_tests[[3]]$p.value))
kable(results_df, digits = 3, format = "html")

```

### Hard shell vs. accessory for within-range RM targets

```{r}
# Read in results for RM targets
df_4 = read.csv('../results/rmes_discrepancies_targets_k4.csv')
df_4_filter = df_4[df_4$PTU %in% ptu$PTU[ptu$pangenome.category=="include" & df_4$discrepancy_selected!="None"],]
df_5 = read.csv('../results/rmes_discrepancies_targets_k5.csv')
df_5_filter = df_5[df_5$PTU %in% ptu$PTU[ptu$pangenome.category=="include" & df_5$discrepancy_selected!="None"],]
df_6 = read.csv('../results/rmes_discrepancies_targets_k6.csv')
df_6_filter = df_6[df_6$PTU %in% ptu$PTU[ptu$pangenome.category=="include" & df_6$discrepancy_selected!="None"],]

# Convert discrepancies to numeric (presence of None has messed this up)
df_4_filter$discrepancy_selected = as.numeric(df_4_filter$discrepancy_selected)
df_5_filter$discrepancy_selected = as.numeric(df_5_filter$discrepancy_selected)
df_6_filter$discrepancy_selected = as.numeric(df_6_filter$discrepancy_selected)

wilcox_tests = list(wilcox.test(df_4_filter$discrepancy_selected, df_4_filter$discrepancy_others, paired = TRUE),
                    wilcox.test(df_5_filter$discrepancy_selected, df_5_filter$discrepancy_others, paired = TRUE),
                    wilcox.test(df_6_filter$discrepancy_selected, df_6_filter$discrepancy_others, paired = TRUE))
results_df = data.frame(n_PTUs = c(nrow(df_4_filter),
                                   nrow(df_5_filter),
                                   nrow(df_6_filter)),
                        median_n_targets = c(median(df_4_filter$n_selected_kmers),
                                   median(df_5_filter$n_selected_kmers),
                                   median(df_6_filter$n_selected_kmers)),
  median_discrepancy_targets=c(median(df_4_filter$discrepancy_selected),
                                        median(df_5_filter$discrepancy_selected),
                                        median(df_6_filter$discrepancy_selected)),
           median_discrepancy_others=c(median(df_4_filter$discrepancy_others),
                                        median(df_5_filter$discrepancy_others),
                                        median(df_6_filter$discrepancy_others)),
           wilcoxon_stat=c(wilcox_tests[[1]]$statistic,
                           wilcox_tests[[2]]$statistic,
                           wilcox_tests[[3]]$statistic),
           p_value=c(wilcox_tests[[1]]$p.value,
                     wilcox_tests[[2]]$p.value,
                     wilcox_tests[[3]]$p.value))
kable(results_df, digits = 3, format = "html")
```

## Analysis of leading region

Some preliminary statistics about discarded PTUs (for main text). 

```{r}
lr = read.csv('../data/leading_region_081123.tsv', sep='\t')
plasmid.table = read.csv('../data/plasmid_table.tsv', sep='\t')
retained.PTUs.lr = read.csv('../data/leading-region-PTUs.txt', header=F)$V1
lr.identified.PTU.names = sort(unique(lr$PTU))
# Minimum sizes
PTU.min.sizes = sort(sapply(lr.identified.PTU.names, function(x) 
  min(plasmid.table[plasmid.table$PTU==x, "Size"])))
```

We discard those where the minimum size is <20kb: E1, E3, E9, E10, E14, E76. We check the maximum size per PTU:

```{r}
sort(sapply(paste0("PTU-E", c("1", "3", "9", "10", "14", "76")), function(x) 
  max(plasmid.table[plasmid.table$PTU==x, "Size"])))
```

These PTUs are retained for analysing the leading region:

```{r}
leading.region.PTUs = sort(names(PTU.min.sizes[PTU.min.sizes>20000]))
cat(leading.region.PTUs, sep="\n")
```

For convenience for other scripts, I save some files with names of plasmids within each PTU.

```{r}
for (PTU in unique(plasmid.table$PTU)){
  cat(plasmid.table$ID[plasmid.table$PTU==PTU], sep="\n",
      file=paste0('~/Downloads/trieste/', PTU, '_plasmids.txt'))
}

```
### Sliding window analysis

k=6 
```{r}
# Theoretical expectation for 6-bp palindrome density
theoreticalPalindromeExpectation = function(p){# p is GC content
  return ((p**2 - p + 1/2)**3)
}
d = read.csv('../test_output_window5000_step500.csv', header=F, stringsAsFactors = F)
colnames(d) = c("PTU", "plasmid", "x", "window_size", "palindrome_count", "palindrome_density", "GC_n")

d.reduced = d[which(d$PTU %in% leading.region.PTUs), ]

d.reduced$expected_density = theoreticalPalindromeExpectation(d.reduced$GC_n/d.reduced$window_size)

d.reduced$diff_density = d.reduced$palindrome_density-d.reduced$expected_density


# One line per plasmid
d.reduced.median = d.reduced %>% group_by(PTU, x) %>% 
  summarise(median=median(diff_density),
            n=length(diff_density), 
            median.original=median(palindrome_density),
            GC=median(GC_n))

# Maximum counts
d.reduced.median.plot = d.reduced.median %>% 
  group_by(PTU) %>%
  mutate(max_n = max(n, na.rm = TRUE),  # Calculate max 'n' for each 'PTU'
         normalized_n = n / max_n * 100) %>%  # Normalize 'n' by this max value
  ungroup() %>%  # Remove the grouping
  select(-max_n)  # Optionally remove the max_n column if not needed
# Plot the distribution
p.lines.expectation = ggplot(d.reduced.median.plot, aes(x, median*1000))+
  geom_hline(yintercept = 0, linetype='dashed')+
  theme_bw()+
  geom_path(aes(group=PTU, alpha=normalized_n))+
  facet_wrap(~PTU, scales="free_x", nrow=3)+
  ylab("Palindrome depletion per kb (5kb sliding window)")+
  labs(alpha="% of plasmids within PTU")+
  theme(panel.grid=element_blank())+  
  scale_x_continuous(labels = function(x) x / 1000) +
  xlab("Position on plasmid (kb)")+
  theme(legend.position = c(0.8, 0.2))
ggsave(p.lines.expectation, file='../results/6mer-deviation-expectation.pdf',
       width=10, height=6)
# Plot only first 25kb
ggsave(p.lines.expectation+scale_x_continuous(labels = function(x) x / 1000, limits=c(0,50000)), file='../results/6mer-deviation-expectation-0-50-kb.pdf',
       width=10, height=6)

```

### Compare leading region to rest of plasmid

```{r}
# Start from d.reduced, which contains per-plasmid results with 5000bp sliding window and step size of 500bp
# Compare depletion in first 5kb with average for the rest of the plasmid
diff.leading.region.average = d.reduced %>% 
  group_by(plasmid, PTU) %>%
  mutate(diff.count=palindrome_count[x=="0"]-mean(palindrome_count),
         diff.density=diff_density[x=="0"]-mean(diff_density)) %>%
  filter(x=="0")

# Order the PTUs by mean values of depletion for better visual display
mean.values = diff.leading.region.average %>% group_by(PTU) %>%
  summarise(mean=mean(diff.density)*1000)
diff.leading.region.average$PTU = ordered(gsub("PTU-", "", diff.leading.region.average$PTU),
                                          levels=gsub("PTU-", "", mean.values$PTU[order(mean.values$mean)]))
p.leading.region.diff.density = ggplot(diff.leading.region.average, aes(as.factor(PTU), diff.density*1000))+ # depletion per-kb rather than per-bp
  ggbeeswarm::geom_quasirandom(alpha=0.8)+
  theme_bw()+
  theme(panel.grid = element_blank())+
  geom_hline(yintercept = 0)+
  xlab("")+
  theme(axis.text=element_text(colour="black", size=18))+
  ylab("Palindrome depletion in leading region (per kb)")+
  theme(axis.text.x=element_text(angle=45, hjust=1))+
  ylab("")
p.leading.region.diff.density
# Based on this, remove two outliers
p.leading.region.diff.density = p.leading.region.diff.density+
  scale_y_continuous(breaks=seq(-8, 4, 2), limits=c(-8,5))
ggsave(p.leading.region.diff.density, file='../results/6mer-deviation-leading-region-vs-rest-of-plasmid-density.pdf',
       width=5.5, height=6)
```
### Averaging over all plasmids

Similar to Samuel et al.'s display of anti-defence gene density, average over all conjugative plasmids.

```{r}
overall.median = d.reduced %>% filter(x<50000) %>%
  group_by(x) %>%
  summarise(y=median(diff_density),
            n=length(diff_density))
p.overall.average = ggplot(overall.median, aes(x, y*1000))+
  geom_line()+
  scale_y_continuous(breaks=seq(-10, 0, 2), limits = c(-9, 0))+
  theme_bw()+
  theme(panel.grid = element_blank())+
  geom_hline(yintercept = 0, linetype='dashed')+
  ylab("Palindrome depletion (per kb)")+
    scale_x_continuous(labels = function(x) x / 1000) +
  xlab("Position on plasmid (kb)")+
  theme(axis.text=element_text(colour="black"))
p.overall.average
# Save
ggsave(p.overall.average, file='../results/6mer-deviation-overall.pdf', 
       width=3, height=3)
```

```{r}
lagging.df = read.csv('~/Downloads/trieste/leading_lagging/NZ_LT994833.1_lagging_10000_k6.csv', header=T)
leading.df = read.csv('~/Downloads/trieste/leading_lagging/NZ_LT994833.1_leading_10000_k6.csv', header=T)
merged.df = merge(leading.df, lagging.df, by="word")
palindromes = tolower(read.csv('~/Downloads/trieste/k6.txt', header=F)$V1)
ggplot(merged.df[which(merged.df$word %in% palindromes),], aes(score.x, score.y))+
  geom_point()
merged.df.palindromes = merged.df[which(merged.df$word %in% palindromes),]
table(merged.df.palindromes$score.x<merged.df.palindromes$score.y)
kmer_targets = tolower(read.csv('~/Downloads/trieste/PTU-LM_targets.txt', header=F)$V1)
ggplot(merged.df[which(merged.df$word %in% kmer_targets),], aes(score.x, score.y))+
  geom_point()
merged.df.targets = merged.df[which(merged.df$word %in% kmer_targets),]
table(merged.df.targets$score.x<merged.df.targets$score.y)

```

### Palindromes in leading vs. lagging

```{r}
scores_df = read.csv('../test_6mer_leading_lagging_scores.csv', header=T)
scores_average <- scores_df %>%
  rowwise() %>%
  mutate(score_avg = mean(c_across(starts_with("score")))) %>%
  ungroup() %>%
  select(kmer, score_avg)  
# Order by average score
scores_average$kmer = ordered(scores_average$kmer,
                              levels=scores_average$kmer[order(scores_average$score_avg)])
scores.plot = ggplot(scores_average, aes(kmer, score_avg))+geom_point()+
  coord_flip()

# Same for counts
counts_df = read.csv('../test_6mer_leading_lagging_counts.csv', header=T)
counts_average <- counts_df %>%
  rowwise() %>%
  mutate(count_avg = mean(c_across(starts_with("count")))) %>%
  ungroup() %>%
  select(kmer, count_avg)  

# Merge together
merge_scores_counts = merge(counts_average, scores_average, by="kmer")
# average score and average count are well-correlated
ggplot(merge_scores_counts, aes(count_avg, score_avg))+geom_point()

# Order by average score
merge_scores_counts$kmer = ordered(merge_scores_counts$kmer,
                              levels=scores_average$kmer[order(scores_average$score_avg)])
scores.plot = ggplot(merge_scores_counts, aes(score_avg, kmer))+
    geom_vline(xintercept = 0)+
  geom_bar(stat="identity")+
  geom_vline(xintercept = median(merge_scores_counts$score_avg), linetype='dashed', colour='red')+
  theme_bw()+
  theme(axis.text=element_text(colour="black", size=12),
        axis.title.x=element_text(size=24),
        axis.text.x=element_text(size=18))+
  theme(axis.text.y=element_text(hjust=0))+
  ylab("")+
  xlab("Mean depletion score\n(leading vs. lagging, 10kb)")
wilcox.test(merge_scores_counts$score_avg)
ggsave(scores.plot, file='../results/6mer-palindromes-scores.pdf',
       height=10, width=10)
```

Test for a particular PTU.

```{r}

results = NULL
for (PTU in leading.region.PTUs){
  for (k in c(5,6)){
    scores_df = read.csv(paste0('../test_target_leading_lagging/', PTU, '_k', k, '_scores.csv'), header=T)
    n_targets = nrow(scores_df)
    scores_average <- scores_df %>%
  rowwise() %>%
  mutate(score_avg = mean(c_across(starts_with("score")))) %>%
  ungroup() %>%
  select(kmer, score_avg)
    counts_df = read.csv(paste0('../test_target_leading_lagging/', PTU, '_k', k, '_counts.csv'), header=T)
    counts_average <- counts_df %>%
  rowwise() %>%
  mutate(count_avg = mean(c_across(starts_with("count")))) %>%
  ungroup() %>%
  select(kmer, count_avg)
    results = rbind(results, c(PTU, k, n_targets, median(scores_average$score_avg), median(counts_average$count_avg)))
  }
}

results = data.frame(results)
colnames(results) = c("PTU", "k", "n.targets", "median.score", "median.count")
results$median.score= as.numeric(results$median.score)
results$median.count = as.numeric(results$median.count)

# Order by median score for k=6
order.k.6.scores = results[which(results$k==6), "PTU"][order(results[which(results$k==6), "median.score"])]
results$PTU = ordered(results$PTU,
                      levels=rev(order.k.6.scores))
# Add number of targets to each

p.depletion.targets = ggplot(results, aes(median.score, as.factor(PTU)))+
  geom_vline(xintercept = 0, size=1)+
  geom_bar(stat="identity", position="dodge")+
  facet_wrap(~k, scales="free_x")+
  ylab("")+
  xlab("Median depletion score of within-range RM targets\n(leading vs. lagging, 10kb)")+
   theme_bw()+
  theme(panel.grid = element_blank())+
  theme(axis.text=element_text(colour="black", size=12),
        axis.title.x=element_text(size=24),
        axis.text.x=element_text(size=18))+
  theme(axis.text.y=element_text(hjust=0))
ggsave(p.depletion.targets, file='../results/figure-depletion-within-range-targets.pdf',
       width=10, height=6)

# Is there a correlation between k=5 and k=6 results?
results.df = results %>% select(k, median.score, PTU) %>%
  pivot_wider(
    names_from = k,  # Use values of 'k' as new column names
    values_from = median.score  # Use 'median.score' for the column values
)
```

